# 安装目录 
software_dir: '/data/k8s_install/k8s_1.24.2'
k8s_work_dir: '/opt/kubernetes'
k8s_addons_dir: '{{ k8s_work_dir }}/addons'
etcd_work_dir: '/opt/etcd'
tmp_dir: '/tmp/k8s'

#cfssl工具:arm/x86
cfssl_type: 'arm'

# 集群网络
service_cidr: '10.0.0.0/24'
cluster_dns: '10.0.0.2'   # 与roles/addons/files/coredns.yaml中IP一致，并且是service_cidr中的IP
pod_cidr: '10.244.0.0/16' # 与calico/flannel/kubeovn中网段一致
pod_cidr_gw: '10.244.0.1'
service_nodeport_range: '30000-32767'
cluster_domain: 'cluster.local'
#cluster_dns: "{{ service_cidr.split('.')[0] }}.{{ service_cidr.split('.')[1] }}.{{ service_cidr.split('.')[2] }}.{{ service_cidr.split('.')[3]|int + 2 }}"
#pod_cidr_gw: "{{ pod_cidr.split('.')[0] }}.{{ pod_cidr.split('.')[1] }}.{{ pod_cidr.split('.')[2] }}.{{ pod_cidr.split('.')[3]|int + 1 }}"

# 高可用，如果部署单Master，该项忽略
vip: '192.168.114.88'
nic: 'ens33'  # 修改为实际内网网卡名

#metallb ip池
metallb_l2_ip_pool: '198.51.100.230-198.51.100.240' #L2最好内网同网段，不然配路由
metallb_BGP_ip_pool: '198.51.100.10/24'
metallb_BGP_peer_ip: '10.0.0.1' #BGP路由器ip

# 自签证书可信任IP列表，为方便扩展，可添加多个预留IP
cert_hosts:
  # 包含所有LB、VIP、Master IP和service_cidr的第一个IP
  k8s:
    - 10.0.0.1
    - 192.168.114.88
    - 192.168.114.30
    - 192.168.114.31
    - 192.168.114.32
    - 192.168.114.33
    - 192.168.114.34
    - 192.168.114.35
    - 192.168.114.36
    - 192.168.114.37
    - 192.168.114.38
    - 192.168.114.39
    - 192.168.114.40
    - 192.168.114.41
    - 192.168.114.42
    - 192.168.114.43
    - 192.168.114.44
    - 192.168.114.45
    - 192.168.114.46
    - 192.168.114.47
    - 192.168.114.48
    - 192.168.114.49
    - 192.168.114.100
    - 192.168.114.101
    - 192.168.114.102
    - 192.168.114.103
    - 192.168.114.104
    - 192.168.114.105
    - 192.168.114.106
    - 192.168.114.107
    - 192.168.114.108
    - 192.168.114.109
    - 192.168.114.110
    - 192.168.114.111
    - 192.168.114.112
    - 192.168.114.113
    - 192.168.114.114
    - 192.168.114.115
    - 192.168.114.116
    - 192.168.114.117
    - 192.168.114.118
    - 192.168.114.119
  # 包含所有etcd节点IP
  etcd:
    - 192.168.114.112
#    - 192.168.114.34
#    - 192.168.114.35

############################
# gpu 
# gpu_label= nsp/sp_t4
# gpu_share_label = nsp/sp_one/sp_mps/sp_time
############################
gpu_interfaces:
  - ip: 192.168.114.112
    gpu_label: nsp
    gpu_share_label: nsp
  - ip: 192.168.114.33
    gpu_label: sp_t4
    gpu_share_label: sp_mps

############################
# kubevirt
############################
kubevirt_install: "yes"
kubevirt_ver: "v1.4.0"

############################
# network
############################
multus_install: "yes"
metallb_install: "yes"
ingress_install: "yes"
dpdk_pre_install: "yes"
sriov_install: "yes"
cluster_network: "calico" #calico/flannel/kube-ovn/
# -------------------------------------------dpdk
dpdk_interfaces:
  - ip: 192.168.114.33
    interface:
      - default_hugepagesz: 1G
        hugepagesz: 1G
        hugepages: 8
  - ip: 192.168.114.112
    interface:
      - default_hugepagesz: 512M
        hugepagesz: 512M
        hugepages: 20
# -------------------------------------------sriov
#启用sriov后，dpdk的vf驱动使用vfio-pci
sriov_interfaces:
  - ip: 192.168.114.33
    interface:
      - name: enp11s0f0
        vf_count: 4
      - name: enp11s0f1
        vf_count: 4
    ixgbe_max_vfs: 8
    pci_map: "/root/.pci-eth" #有此配置会更换vf驱动为vfio-pci(need install python3)
  - ip: 192.168.114.112
    interface:
      - name: enp43s0f3
        vf_count: 4
      - name: enp35s0f0np0
        vf_count: 4
    ixgbe_max_vfs: 8

sr_vf_vendors: '["8086", "14e4"]'
sr_vf_devices: '["154c", "10ed", "1889", "16dc", "1520"]'
sr_vf_drivers: '["iavf", "i40evf", "igbvf", "ixgbevf", "bnxt_en", "vfio-pci"]'
sr_pf_name: '["enp43s0f3", "enp35s0f0np0"]' #all ip-name
# -------------------------------------------kube-ovn
kubeovn_ver: "v1.13.2" #v1.11.5
#kubeovn_loadbalancer_enable: false
kubeovn_cilium_enable: true
kubeovn_cilium_ui_svc_type: "NodePort"
kubeovn_cilium_ui_nodeport: 31235
# ------------------------------------------- flannel
# [flannel]设置flannel 后端"host-gw","vxlan"等
flannel_backend: "vxlan"
direct_routing: false #支持vxlan在相同子网情况下数据包直接通过路由转发,与HOST-GW模式相同
# ------------------------------------------- calico
calico_ver: "v3.26.4"
# [calico]设置calico 网络 backend: bird, vxlan, none
calico_networking_backend: "bird"
# [calico] IPIP隧道模式可选项有: [Always, CrossSubnet, Never],跨子网可以配置为Always与CrossSubnet(公有云建议使用always比较省事，其他的话需要修改各自公有云的网络配置，具体可以参考各个公有云说明)
# 其次CrossSubnet为隧道+BGP路由混合模式可以提升网络性能，同子网配置为Never即可.
calico_ipv4pool_ipip: "Always"
calico_vxlan_ipip: "Never"  
# [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现
ip_autodetection_method: "interface=enp43s0f.*,eno.*,eth.*" #cidr=10.20.0.0/24  can-reach={{ groups['master'][0] }}
# [calico]设置calico 是否使用route reflectors
# 如果集群规模超过50个节点，建议启用该特性
calico_rr_enable: false
# calico_rr_nodes 配置route reflectors的节点，如果未设置默认使用集群master节点 
# calico_rr_nodes: ["192.168.1.1", "192.168.1.2"]#集群内节点
calico_rr_nodes: []
# calico AS number
calico_as_number: 64512
#etcd
tmp_endpoints: "{% for h in groups['etcd'] %}https://{{ h }}:2379,{% endfor %}"
etcd_endpoints: "{{ tmp_endpoints.rstrip(',') }}"

############################
# ipvs
############################
proxy_mode: "ipvs" #不填默认为iptables
enable_ipvs_strict_arp: true #true 设置确保负载均衡器只响应那些针对其实际绑定的 IP 地址的 ARP #集群是使用IPVS模式下kube-proxy，则从kubernetes v.1.14.2开始，必须启用ARP模式
ipvs_scheduler_type: "rr"

############################
# ha高可用(多master)
############################
ha_support: false

############################
# gpu
############################
nvidia_toolkit_ver: "1.16.1"
nvidia_gpu_plugin_ver: "0.16.1"
nvidia_gpu_share_rename: "false"
nvidia_gpu_config_replicas: 10  #mps最大48、time无限制

############################
# nfs server
############################
nfs_default_path: "/data/k8s"

############################
#metrics-server
############################
metrics_server_install: "yes"

############################
#dashboard
#和kubesphere二选一
############################
dashboard_install: "no"
dashboard_nodeport: 30001

############################
# prometheus+grafana
# 和kubesphere二选一
############################
prom_install: "no"
prom_namespace: "monitor"
prom_chart_ver: "45.23.0"
prom_operator_nodeport: 30899
prom_operator_nodeport_tls: 30900
prom_self_nodeport: 30901
prom_alertmanager_nodeport: 30902
prom_grafana_nodeport: 30903

############################
# kubesphere 30880
############################
kubesphere_install: "yes"
kubesphere_ver: "3.3.2"
kubesphere_api_nodeport: 30801
kubesphere_nodeport: 30880
kubesphere_password: "P@88w0rd"
#host官方是有bug的，拆分安装
kubesphere_clusterRole_host: "yes"
kubesphere_clusterRole_others: "none" #no:member(从集群) | yes:none(主机群) |no:none(default)
kubesphere_jwtSecret: "" #从集群时候填充
#查询jwtSecret： kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v "apiVersion" | grep jwtSecret

############################
# helm charts
#和kubeconfig部署在一起
############################
HELM_VER: "v3.11.1"
HELM_PATH: "/root/helm"
HELM_PORT: 80 #harbor和helm部署在一起 端口不可为80
HELM_CHARTS: "private" #public为官方联网

############################
# harbor
############################
HARBOR_VER: "v2.10.2"
HARBOR_DOMAIN: "registry.zjy.com"
HARBOR_PATH: "/var/data"
HARBOR_TLS_PORT: 443
#如果端口等于443，HARBOR_REGISTRY可以不需要HARBOR_TLS_PORT
HARBOR_REGISTRY: "{{ HARBOR_DOMAIN }}"
#HARBOR_REGISTRY: "{{ HARBOR_DOMAIN }}:{{ HARBOR_TLS_PORT }}"
HARBOR_PASSWORD: "zjy@123456"
HARBOR_DB_PASSWORD: "zjydb@123456" 
# install extra component
HARBOR_WITH_TRIVY: false

############################
# role:runtime [containerd,docker]
############################
# [.]docker启用拉取加速镜像仓库
ENABLE_MIRROR_REGISTRY: true

# [.]添加信任的私有仓库
# 必须按照如下示例格式，协议头'http://'和'https://'不能省略
INSECURE_REG:
  - "https://harbor.yourcompany.com"
#  - "http://{{ groups['harbor'][0] }}:8080"

# [containerd]容器持久化存储目录
CONTAINERD_STORAGE_DIR: "/var/lib/containerd"

# [docker]容器存储目录
DOCKER_STORAGE_DIR: "/var/lib/docker"

# [docker]开启Restful API
DOCKER_ENABLE_REMOTE_API: false
